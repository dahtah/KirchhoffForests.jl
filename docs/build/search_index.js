var documenterSearchIndex = {"docs":
[{"location":"trace/#Trace-estimation-via-RSFs","page":"Trace Estimation","title":"Trace estimation via RSFs","text":"","category":"section"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"Trace, i.e. the sum of diagonal entries of an input matrix, is an essential operation in linear algebra and it is of central importance in many applications in machine learning. However, it is not trivial to compute when the diagonals of the input matrix are expensive to compute.","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"Here we give efficient RSF-based estimators for approximating the trace of regularized inverse texttr(mathsfK) which normally requires computing the inverse of a large matrix. They are unbiased, easy to implement and parallelise and can be used for the regularized inverse of symmetric diagonally dominant matrices instead of graph Laplacians. See Simon Barthelmé, Nicolas Tremblay, Alexandre Gaudillière, Luca Avena, Pierre-Olivier Amblard (2019) for more details.","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"note: RSF-based trace estimator\nThe proposed estimator s is the number of roots in Phi_Q:    s coloneqq rho(Phi_Q)which verifies mathbbEs = texttr(mathsfK).","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"One can access the number of roots in the sampled forest as follows:","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"julia> using KirchoffForests,Graphs\n\njulia> g = grid([4,4])\n{16, 24} undirected simple Int64 graph\n\njulia> q = 1.0\n1.0\n\njulia> rf = random_forest(g,q)\nRandom forest. Size of original graph 16.\nNumber of trees 8\n\n\njulia> s = rf.nroots\n8","category":"page"},{"location":"trace/#Variance-Reduction","page":"Trace Estimation","title":"Variance Reduction","text":"","category":"section"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"By applying standard variance reduction techniques in Monte Carlo literature, one reduces the expected error/variance of this estimator.","category":"page"},{"location":"trace/#Control-Variate","page":"Trace Estimation","title":"Control Variate","text":"","category":"section"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"By adapting the control variate in GTR, one obtains the following estimators:","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"    tildescoloneqq s - α*(texttr(mathsfK^-1tildemathsfS) - s)  \n    \n    barscoloneqq s - α*(texttr(mathsfK^-1barmathsfS) - s)","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"where tildemathsfS and barmathsfS are typically low-rank linear operators that verifies tildemathbfx = tildemathsfSmathbfy and barmathbfx = barmathsfSmathbfy, respectively. In other words, when applied to a vector, tildemathsfS propagates the measurements in the roots and barmathsfS propagates the average measurement within each tree. One can choose alpha as suggested in GTR and for more details, see Yusuf Yiğit Pilavcı, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay (2022).","category":"page"},{"location":"trace/#Stratification","page":"Trace Estimation","title":"Stratification","text":"","category":"section"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"Another variance reduction technique we can use for trace estimation is stratification. Imagine a random variable X with an unknown expectation mu. This technique suggests:","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"1 - Collecting N_k conditional samples of XYin C_k for each kin1dotsK where Y is another random variable with a known probability distribution over its sample space Omega =cup_k=1^K C_k,","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"2 - Then approximating mu by using the law of iterated expectation as follows: ","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"    mu_st coloneqq underbracesum_k=1^Kunderbracefrac1N_kleft(sum_j=1 ^N_k X^(j)Yin C_k  right)_textConditional ExpectationmathbbP(Yin C_i)_textMarginalization over  Y","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"For certain allocations of samples N_k's, the theoretical variance of mu_st is less than that of the naive estimator. We can build such a setup for trace estimator s by setting Y to \"the roots that are sampled at the first visits\" of random walks in Wilson's algorithm. These roots are a subset of rho(Phi_q) and their distribution (Poisson-Binom distribution) is tractable by using the degrees of nodes. More details can be found in Yusuf Yigit Pilavci (2022). ","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"All of these techniques are implemented in trace_estimator as different variants. Here is an example comparing all these methods: ","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"using KirchoffForests,Graphs,LinearAlgebra,PyPlot, StatsBase\npygui(true)\nn = 10000\ng = barabasi_albert(n,10)\nL = Matrix(laplacian_matrix(g))\nq = 1.0\ntr_exact = tr(q*inv(L+q*I)) \n\nα=2*q/(2*q+mean(degree(g)))\nNREPRANGE = Int64.(round.(10 .^ (1:0.05:2)))\n\nerr_tr_est = zeros(length(NREPRANGE))\nerr_tr_cv_tilde = zeros(length(NREPRANGE))\nerr_tr_cv_bar = zeros(length(NREPRANGE))\nerr_tr_st = zeros(length(NREPRANGE))\n\nEXPREP = 1000\nfor er = 1 : EXPREP\n    for (idx,NREP) in enumerate(NREPRANGE)\n        tr_est = trace_estimator(g,q;variant=1,NREP=NREP) # The forest estimator s\n        err_tr_est[idx] += (tr_est - tr_exact)^2\n        tr_est_cv_tilde = trace_estimator(g,q;variant=2,α=α,NREP=NREP) # s_tilde \n        err_tr_cv_tilde[idx] += (tr_est_cv_tilde - tr_exact)^2\n        tr_est_cv_bar = trace_estimator(g,q;variant=3,α=α,NREP=NREP) # s_bar\n        err_tr_cv_bar[idx] += (tr_est_cv_bar - tr_exact)^2\n        tr_est_st = trace_estimator(g,q;variant=4,α=α,NREP=NREP) # stratified estimator s_st\n        err_tr_st[idx] += (tr_est_st - tr_exact)^2\n    end\nend\n\nplot(NREPRANGE,err_tr_est ./ EXPREP,label=\"s\")\nplot(NREPRANGE,err_tr_cv_tilde ./ EXPREP,label=\"\\$ \\\\tilde{s}\\$\")\nplot(NREPRANGE,err_tr_cv_bar ./ EXPREP,label=\"\\$ \\\\bar{s}\\$\")\nplot(NREPRANGE,err_tr_st ./ EXPREP,label=\"\\$ s_{st}\\$\")\n\nxlabel(\"Number of Samples\")\nylabel(\"Mean Square Error\")\nyscale(\"log\")\nxscale(\"log\")\nlegend()","category":"page"},{"location":"trace/","page":"Trace Estimation","title":"Trace Estimation","text":"(Image: )","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"rsf/#What's-an-RSF?","page":"What's an RSF?","title":"What's an RSF?","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"In a graph mathcalG=(mathcalVmathcalEw) consisting of nodes mathcalV, edges mathcalE and edge weights w, a tree is a subgraph without cycles, and a forest is a set of trees. This package is focused on generating random spanning forests in a particular way.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"4x4 Grid Graph mathcalG=(mathcalVmathcalEw) A Spanning Tree $ \\tau $ of mathcalG A Spanning Forest $ \\phi $ of mathcalG\n(Image: ) (Image: ) (Image: )","category":"page"},{"location":"rsf/#Random-(rooted)-spanning-forests","page":"What's an RSF?","title":"Random (rooted) spanning forests","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Importantly, all the forests we use are considered to be rooted: each tree in the forest is directed, and all edges point towards the root of the tree. In the root and forest above, the roots are yellow.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"When we talk about a \"random spanning forest\", denoted by $ \\Phi_q $, we mean a forest $ \\phi $ sampled from the following distribution:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"mathbbP(Phi_q = phi)=frac1Z_q q^rho(phi) prod_(ij)in phi w(ij)","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"$ Z_q $ is the normalization constant such that:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":" sum_phimathbbP(Phi_q = phi)=1","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"$ \\phi $ is a fixed forest,\n$ \\rho(\\phi) $ is the root set of $ \\phi $,\n$ w(i,j) $ is the weight associated with the edge (i,j) (which equals 1 if the graph is unweighted)\n$ q > 0$ is a scalar that determines the average number of trees.","category":"page"},{"location":"rsf/#Wilson's-algorithm-for-sampling-\\Phi_q","page":"What's an RSF?","title":"Wilson's algorithm for sampling Phi_q","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Wilson's algorithm, proposed by David Bruce Wilson (1996), is a simple and efficient algorithm to sample Phi_q. It is based on loop-erased random walks that use a Markov chain X with the following transition rule at step n:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"    mathbbP(X(n+1)=jX(n) = i) = begincases\n          fracw(ij)d_i+q  ijnot=Gamma  \n          fracqd_i+q   inot=j=Gamma \n          1   i=Gamma \n    endcases","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where Gamma is the additional absorption state.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Wilson's algorithm operates as follows:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Initialize the sets Delta=Gamma and phi=emptyset,\nStart X from an arbitrary node in mathcalVsetminusDelta and interrupt whenever X(n)inDelta,\nErase the loops in X as they appear to obtain a path gamma,\nUpdate phi leftarrow phicupgamma to phi and Delta leftarrow Deltacup s(gamma),\nIf mathcalVsetminusDelta is an empty set, return phi. Otherwise, go to step 2.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"The following animation illustrates Wilson's algorithm on a 4x4 grid graph for sampling RSFs.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"<table>\n<tr>\n<th align=\"center\">\n<img align=\"center\" src= ../wilsons_gif.gif width=\"400\" height=\"400\">  \n</th>\n</tr>\n<tr>\n<th>\n<ul><li> <a style=\"color:green\">Green</a>: Current state of the random walker</ul></li> <ul><li><a style=\"color:yellow\">Yellow</a>: Roots of the sampled forest </ul></li> <ul><li> <a style=\"color:darkblue\">Dark Blue</a>: Nodes that are attached to a branch of the sampled forest</ul></li> <ul><li> <a style=\"color:lightblue\">Blue</a>: Nodes that have not been attached to the forest. </ul></li>\n</th>\n</tr>\n</table>","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"In KirchoffForests.jl, one can call this algorithm as follows:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"julia> using KirchoffForests,Graphs\n\njulia> q = 1.0\n1.0\n\njulia> g = grid([4,4])\n{16, 24} undirected simple Int64 graph\n\njulia> rf = random_forest(g,q)\nRandom forest. Size of original graph 16.\nNumber of trees 6","category":"page"},{"location":"rsf/#Value-of-q-vs.-the-number-of-roots","page":"What's an RSF?","title":"Value of q vs. the number of roots","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Here are samples of $ \\Phi_q $ for different values of $ q $ :","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"$ q = 0.1$ $ q = 1.0$ $ q = 5.0$\n(Image: ) (Image: ) (Image: )","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"In a more generalized form of this distribution, one can consider varying values of q over the vertices. In this case, we define a more generic distribution as follows:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"mathbbP(Phi_Q = phi)=frac1Z_Q prod_(r)inrho(phi)q_r prod_(ij)in phi w(ij)","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where Q=(q_1dotsq_n)inmathbbR^mathcalV and Z_Q is the corresponding normalization constant. One can sample such RSFs via Wilson's algorithm by adapting the transition probability as","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"    mathbbP(X(n+1)=jX(n) = i) = begincases\n          fracw(ij)d_i+q_i  ijnot=Gamma  \n          fracq_id_i+q_i   inot=j=Gamma \n          1   i=Gamma \n    endcases","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"This is also implemented in this package:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"julia>  using KirchoffForests,Graphs\n\njulia> g = grid([4,4])\n{16, 24} undirected simple Int64 graph\n\njulia> q = rand(16)\n16-element Array{Float64,1}:\n 0.9977096157204421\n 0.7109113019106763\n 0.006338471270104895\n 0.6482324667545591\n 0.7955474196350312\n 0.5820420279765701\n ⋮\n 0.9302933097261235\n 0.5396398577920467\n 0.39650118327620065\n 0.09064936914066157\n 0.9284607656864134\n 0.6703257960094731\n\njulia>  rf = random_forest(g,q)\nRandom forest. Size of original graph 16.\nNumber of trees 3","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Non-homogenous q values over the vertices reweight the probability of being a root for each vertex. Higher q_i makes node i more likely to be a root in an RSF.","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Uniform $ q $ over vertices Varying $ q $ over vertices\n(Image: ) (Image: )","category":"page"},{"location":"rsf/#Links-with-Graph-Laplacian","page":"What's an RSF?","title":"Links with Graph Laplacian","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Given a graph mathcalG=(mathcalVmathcalEw), graph Laplacian matrix mathsfLinmathbbR^mathcalVtimes mathcalV is:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"    mathsfL_ij coloneqq begincases\n          -w(ij)   inot=j \n          sum_kinmathcalN(i)w(ik)  textotherwise \n    endcases","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where mathcalN(i) denotes the neighbors of node i in mathcalG.","category":"page"},{"location":"rsf/#Matrix-Forest-Theorem","page":"What's an RSF?","title":"Matrix-Forest Theorem","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"An immediate link between Phi_Q and mathsfL is shown by Gustav Kirchhoff (1847) as","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"    Z_Q =  det (mathsfQ + mathsfL)mathsfQ","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where mathsfQ=textdiag(q_1dotsq_n) and Z_Q is the normalization constant for Phi_Q.","category":"page"},{"location":"rsf/#Root-set-of-\\Phi_Q-is-a-DPPs","page":"What's an RSF?","title":"Root set of Phi_Q is a DPPs","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"Another well-known result is that rho(Phi_Q) is a determinantal point process that verifies:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"  forall SinmathcalVquad mathbbP(S subseteq rho(Phi_Q)) =  det mathsfK_S","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"where mathsfK= (mathsfQ + mathsfL)^-1mathsfQ and mathsfK_S is the sub-matrix where mathsfK is restricted to the columns and rows indexed by $ S $. See Robert Burton, Robin Pemantle (1993) for more details.","category":"page"},{"location":"rsf/#Probability-of-i-rooted-in-j","page":"What's an RSF?","title":"Probability of i rooted in j","text":"","category":"section"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"A bit less intuitive, but interesting link is about the root relation in Phi_q. Define a function r_phimathcalVrightarrowrho(phi) that maps every node iinmathcalV to its root in phi. Then, the probability of having i rooted in j in Phi_q reads:","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"  mathbbP(r_Phi_Q(i) = j) = mathsfK_ij text with  mathsfK= (mathsfQ + mathsfL)^-1mathsfQ","category":"page"},{"location":"rsf/","page":"What's an RSF?","title":"What's an RSF?","text":"See Luca Avena, Fabienne Castell, Alexandre Gaudillière, Clothilde Mélot (2018) for more details.","category":"page"},{"location":"gtr/#Graph-signal-smoothing-and-interpolation","page":"RSF-based Graph Tikhonov Regularization","title":"Graph signal smoothing and interpolation","text":"","category":"section"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Signal denoising (i.e. eliminating noisy components in the signal) and impainting (i.e. completing) are two well-known problems in classical signal processing. In the case of graph signals (i.e. signals that are defined over the vertices), various approaches aim to output a solution that is a smooth signal (i.e. not highly varying through the edges) over the underlying graph structure.  ","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Graph Tikhonov regularization is one of these approaches that generically formulates the problem:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"note: Graph Tikhonov regularization\nGiven a graph mathcalG=(mathcalVmathcalEw), assume we have pmathcalV noisy measurements mathbfy=y_1dotsy_p^top over the vertices. We would like to obtain a smooth and complete signal over the vertices by solving:hatmathbfx = argmin_mathbfx in mathbbR^n underbrace mathsfMmathbfy - mathbfx ^2_mathsfQ_textFidelity +underbrace mathbfx^TmathsfLmathbfx_textRegularizationwhere mathsfMinmathbbR^ntimes p is a partial identity matrix, mathsfQ=textdiag(q_1dotsq_n) contains the pointwise regularization parameters, mathsfL  is the graph Laplacian and mathbfx^TmathsfLmathbfx  = sumlimits_(ij)inmathcalEw(ij)(x_i-x_j)^2. The closed-form solution to this formulation is:    hatmathbfx  = mathsfKmathbfywith mathsfK=(mathsfL + mathsfQ)^-1mathsfQ and mathbfy= mathsfMmathbfy.","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"There is an unbiased estimator to estimate this matrix inverse via RSFs. This estimator, defined as tildex_i coloneqq y_r_Phi_Q(i), operates as follows:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Sample Phi_Q by setting Q = (q_1 dotsq_n),\nThen, within each tree of the sampled forest, propagate the measurement in the root.","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"See Yusuf Yiğit Pilavcı, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay (2021) for more details.","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"This estimator can be called in KirchoffForests.jl as follows:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"julia> using KirchoffForests,Graphs,LinearAlgebra,Random,PyPlot\n\njulia> rng = MersenneTwister(12345); # Set random seed\n\njulia> g = Graphs.grid([4,4])\n{16, 24} undirected simple Int64 graph\n\n# Generate an incomplete signal\njulia> p = 8; n = nv(g); y= rand(rng, p); labelednodes = randperm(rng,n)[1:p]; M=I(n)[:,labelednodes];\n\njulia> yprime = M*y;\n\njulia> Q = rand(rng, n); # Set regularization parameters\n\njulia> rf = random_forest(g,Q,rng)\nRandom forest. Size of original graph 16.\nNumber of trees 4\n\n\njulia> xtilde = rf*yprime\n16-element Array{Float64,1}:\n0.0\n0.0\n0.0\n0.0\n0.8350140149860443\n0.8350140149860443\n0.8350140149860443\n⋮\n0.8350140149860443\n0.8350140149860443\n0.36580119057192695\n0.8350140149860443\n0.8350140149860443\n0.36580119057192695\n0.36580119057192695","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Input graph and signal mathbfy Sampled forest rf The RSF estimator tildemathbfx\n(Image: ) (Image: ) (Image: )","category":"page"},{"location":"gtr/#Variance-reduction-on-\\tilde{\\mathbf{x}}","page":"RSF-based Graph Tikhonov Regularization","title":"Variance reduction on tildemathbfx","text":"","category":"section"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"There are several ways to reduce the variance of tildemathbfx:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Conditional Monte Carlo: Thanks to the nice theoretical properties of RSFs, one can reduce the variance of tildemathbfx by propagating the locally weighted averages within each tree instead of the measurement of the root:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"    barx_i = fracsumlimits_jinmathcalV_t(i)q_j y_jsumlimits_kinmathcalV_t(i)q_k","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"where mathcalV_t(i) is the set of vertices of the tree in Phi_Q which node i belongs to. See Yusuf Yiğit Pilavcı, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay (2021) for more details.","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"This new estimator barmathbfx can be computed by using the structure called Partition:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"julia> p = Partition(rf)\nGraph partition. Size of original graph 16.\nNumber of parts 4\n\n# A bit elaborate way to compute the weighted average within each tree\njulia> xbar = (p*(yprime .* Q)) ./ (p*Q)\n16-element Array{Float64,1}:\n 0.1995397697360063\n 0.1995397697360063\n 0.1995397697360063\n 0.1995397697360063\n 0.29575020044545924\n 0.29575020044545924\n ⋮\n 0.29575020044545924\n 0.07777354234714352\n 0.29575020044545924\n 0.29575020044545924\n 0.07777354234714352\n 0.07777354234714352","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"The estimate by tildemathbfx The estimate by barmathbfx Exact Solution hatmathbfx\n(Image: ) (Image: ) (Image: )","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Control variate technique: Another technique to reduce the variance/expected error is the control variate technique which defines the following estimator:","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"barmathbfz coloneqq barmathbfx - alpha(mathsfK^-1barmathbfx - mathbfy)","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"where alpha is a hyperparameter which is easy to choose as described in Yusuf Yiğit Pilavcı, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay (2021).   ","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"julia> L = laplacian_matrix(g);\n\njulia> α = 2*minimum(Q)/(minimum(Q)+maximum(degree(g))) # Chosen as suggested\n0.009474493197967612\n\njulia> zbar = xbar - α*((L * xbar) ./ Q + xbar  - yprime)\n16-element Array{Float64,1}:\n 0.20359013809375195\n 0.25085798950864824\n 0.1999698865149873\n 0.19439266976895148\n 0.29449512115874615\n 0.2912856157260843\n ⋮\n 0.28991581536522015\n 0.0791621243376845\n 0.2929481171830412\n 0.29329844131810934\n 0.0907881208925323\n 0.07703667744919367","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"The estimate by barmathbfx The estimate by barmathbfz Exact Solution hatmathbfx\n(Image: ) (Image: ) (Image: )","category":"page"},{"location":"gtr/#Some-other-applications","page":"RSF-based Graph Tikhonov Regularization","title":"Some other applications","text":"","category":"section"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"We find some applications of these estimators in certain graph-based optimization problems: ","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"Solving Edge-LASSO via ADMM. Notebook\nNewton's method for Poisson noise Notebook","category":"page"},{"location":"gtr/","page":"RSF-based Graph Tikhonov Regularization","title":"RSF-based Graph Tikhonov Regularization","text":"More explanations and examples are in the notebooks. ","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/#Quasi-Newton's-method-by-RSFs","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"","category":"section"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"In this notebook, we illustrate a use of RSF-based estimators for mathsfKmathbfy. In this case, we look at the following problem where the data fidelity term is a convex, differentiable function f_mathbfyqmathbbR^nrightarrowmathbbR:","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"$","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"\\mathbf{x}^* = \\arg\\min_{x\\in\\mathbb{R}^{n}} L(\\mathbf{x}),","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"$","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"where the loss function L reads:  $     L(x) = f_{\\mathbf{y},q}(\\mathbf{x}) + \\frac{1}{2}\\mathbf{x}^\\top\\mathsf{L}\\mathbf{x}. $","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Newton's method gives the following iteration for approximating mathbfx^*: $     \\mathbf{x}{k+1} =\\mathbf{x}{k} -\\alpha \\mathsf{H}^{-1}\\nabla L(\\mathbf{x}), $ where nabla is the gradient operator and mathsfH is the Hessian matrix (matrix of partial derivatives) of L(x) that verifies: $     \\mathsf{H}=\\left[\\frac{\\partial^2 L(\\mathbf{x})}{\\partial{xi}\\partial {xj}}\\right]_{i,j} $","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Assume fracpartial^2 L(mathbfx)partialx_ipartial x_j = 0 for all inot=j and collect the non-zero second derivatives into a vector mathbfg=leftfracpartial^2 L(mathbfx)partialx_i^2 right_i in mathbbR^n. One recovers the regularized Laplacian as the Hessian matrix: ","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"$","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"\\mathsf{H} = \\mathsf{L} + \\text{diag}(\\mathbf{g})","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"$","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"and the gradient becomes: ","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"$","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"\\nabla L(\\mathbf{x}) = \\mathsf{L}\\mathbf{x} + \\mathbf{h}, $","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"where mathbfh=leftfracpartial L(mathbfx)partialx_iright_i collects the first derivatives. As a result, one needs to compute the regularized inverse mathsfH^-1nabla L(mathbfx) which can be efficiently done by RSF-based estimators.","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"using KirchoffForests","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling RandomForests [e81499c0-f9dc-443f-84b6-a65cdedd22fa]","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"using Graphs\nusing StatsBase\nusing Distributions\nusing SparseArrays\nusing LinearAlgebra\nusing PoissonRandom\nusing PyPlot","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"using TestImages, ImageQualityIndexes,Images","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/#Poisson-Model","page":"Quasi-Newton's method by RSFs","title":"Poisson Model","text":"","category":"section"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Here in this example, we assume the noisy image is generated by the Poisson model:  $      \\mathbf{y} \\sim \\text{Poisson}(\\exp(\\mathbf{x}))  $ where exp is the element-wise exponential function. Poisson model is a popular choice in image processing as it naturally models the shot noise of a camera. Having this as the likelihood function, we choose to use a log-likelihood function for f_mathbfyq which takes the following form:  $     f{\\mathbf{y},q} = q \\sum{i=1}^n \\exp(xi) - yi x_i  $","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"imname = \"peppers\"\nimage = Gray.(imresize(testimage(imname),100,100))\nrescale = 25.0\n\nnx = size(image,1)\nny = size(image,2)\nrs = (v) -> reshape(v,nx,ny)\nG = Graphs.grid([nx,ny])\nscaledimage = (rescale .* Float64.(image))\nim_noisy = pois_rand.(scaledimage)\nim_noisy .= Int64.(im_noisy)\n\nsubplot(1,2,1)\ntitle(\"Original Image\")\nimshow(Float64.(image),cmap=\"gray\")\naxis(\"off\")\nsubplot(1,2,2)\ntitle(\"Noisy Image\")\nimshow(im_noisy./rescale,cmap=\"gray\")\naxis(\"off\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"(Image: png)","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"(-0.5, 99.5, 99.5, -0.5)","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"x = zeros(size(image))\nxtilde = zeros(size(image))\nxbar = zeros(size(image))\ny = im_noisy\ny = y[:]\n\nz0 = (rand(nv(G)))\nmu = 0.1\nnrep = 10\nmaxiter = 100\ntol = 10^(-10)","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"1.0e-10","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"# A helper function to compute constant part of the loss function\nfunction logfact(y)\n    res = 0\n    for i = 1 : y\n        res += log(i)\n    end\n    res\nend\nlogysum = sum(logfact.(y))","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"213266.5372516797","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/#Newton's-update","page":"Quasi-Newton's method by RSFs","title":"Newton's update","text":"","category":"section"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Then, Newton's iteration for solving the optimization problem becomes:  $     \\mathbf{x}{k+1} = \\mathbf{x}{k} - \\alpha (\\mathsf{L} + \\mathsf{Q})^{-1}\\mathsf{Q}\\mathbf{y}'k $ with  $     \\mathsf{Q} = q\\text{ diag}(\\exp(\\mathbf{x}k)) \\      \\forall i\\in \\mathcal{V}, \\quad \\mathbf{y}'i =  \\frac{(\\mathsf{L}\\mathbf{x})i + q(\\exp(xi) - yi)}{q\\exp(x_i)}  $ This derivation shows that one needs to compute this regularized inverse at every iteration which can be approximated by RSF-based estimators. The following function implements this iteration for the exact and forest-based updates.","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"t = @elapsed xhat,increment_hat,loss_hat = newton_poisson_noise(G,y,z0,mu;numofiter = maxiter,tol=tol, method=\"exact\")\ndisplay(\"Completed in $t secs\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Iteration=0, ||t_k - t_{k+1}||^2=57.780858510888386, alpha=0.1\nIteration=1, ||t_k - t_{k+1}||^2=159.48889612574527, alpha=0.25\nIteration=2, ||t_k - t_{k+1}||^2=29.67491701365079, alpha=0.5\nIteration=3, ||t_k - t_{k+1}||^2=9.121626702441024, alpha=0.5\nIteration=4, ||t_k - t_{k+1}||^2=3.984170450190693, alpha=0.5\nIteration=5, ||t_k - t_{k+1}||^2=1.8802981749208878, alpha=0.5\nIteration=6, ||t_k - t_{k+1}||^2=0.9151688535808596, alpha=0.5\nIteration=7, ||t_k - t_{k+1}||^2=0.45166452171408245, alpha=0.5\nIteration=8, ||t_k - t_{k+1}||^2=0.22439051913799307, alpha=0.5\nIteration=9, ||t_k - t_{k+1}||^2=0.11183946439677217, alpha=0.5\nIteration=10, ||t_k - t_{k+1}||^2=0.05583135507684987, alpha=0.5\nIteration=11, ||t_k - t_{k+1}||^2=0.027893654204344937, alpha=0.5\nIteration=12, ||t_k - t_{k+1}||^2=0.013941330105204196, alpha=0.5\nIteration=13, ||t_k - t_{k+1}||^2=0.006969291905954821, alpha=0.5\nIteration=14, ||t_k - t_{k+1}||^2=0.0034843028040168838, alpha=0.5\nIteration=15, ||t_k - t_{k+1}||^2=0.0034841312639460205, alpha=1.0\nIteration=16, ||t_k - t_{k+1}||^2=3.371995435568249e-8, alpha=0.5\nIteration=17, ||t_k - t_{k+1}||^2=1.6859977304575762e-8, alpha=0.5\nMethod: exact. Terminated after 18 iterations, increment 6.563286966583503e-14\n\n\n\n\"Completed in 1241.637454784 secs\"","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"t = @elapsed xtilde,increment_tilde,loss_tilde = newton_poisson_noise(G,y,z0,mu;numofiter = maxiter,tol=tol, method=\"xtilde\",nrep=nrep)\ndisplay(\"Completed in $t secs\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Iteration=0, ||t_k - t_{k+1}||^2=57.780858510888386, alpha=0.1\nIteration=1, ||t_k - t_{k+1}||^2=86.5129913711386, alpha=0.125\nIteration=2, ||t_k - t_{k+1}||^2=39.548220635374705, alpha=0.125\nIteration=3, ||t_k - t_{k+1}||^2=55.3721264955477, alpha=0.25\nIteration=4, ||t_k - t_{k+1}||^2=32.45930329366441, alpha=0.25\nIteration=5, ||t_k - t_{k+1}||^2=26.312071497472854, alpha=0.25\nIteration=6, ||t_k - t_{k+1}||^2=10.962428516750004, alpha=0.125\nIteration=7, ||t_k - t_{k+1}||^2=16.090191379513506, alpha=0.25\nIteration=8, ||t_k - t_{k+1}||^2=8.070568546619528, alpha=0.0625\nIteration=9, ||t_k - t_{k+1}||^2=8.95481590023744, alpha=0.125\nIteration=10, ||t_k - t_{k+1}||^2=5.845994895280066, alpha=0.0625\nIteration=11, ||t_k - t_{k+1}||^2=7.09187367438066, alpha=0.125\nIteration=12, ||t_k - t_{k+1}||^2=6.738825308567337, alpha=0.125\nIteration=13, ||t_k - t_{k+1}||^2=5.492150438660859, alpha=0.125\nIteration=14, ||t_k - t_{k+1}||^2=3.766397090403906, alpha=0.03125\nIteration=15, ||t_k - t_{k+1}||^2=5.053862757366465, alpha=0.125\nIteration=16, ||t_k - t_{k+1}||^2=5.222759564106757, alpha=0.125\nIteration=17, ||t_k - t_{k+1}||^2=4.491994080996922, alpha=0.0625\nIteration=18, ||t_k - t_{k+1}||^2=4.950076863327948, alpha=0.125\nIteration=19, ||t_k - t_{k+1}||^2=3.1496051318625877, alpha=0.125\nIteration=20, ||t_k - t_{k+1}||^2=1.9976896984770434, alpha=0.03125\nIteration=21, ||t_k - t_{k+1}||^2=1.8727606167053, alpha=0.03125\nIteration=22, ||t_k - t_{k+1}||^2=1.4450384148924305, alpha=0.03125\nIteration=23, ||t_k - t_{k+1}||^2=3.977749324671333, alpha=0.125\nIteration=24, ||t_k - t_{k+1}||^2=2.978994051552361, alpha=0.125\nIteration=25, ||t_k - t_{k+1}||^2=2.146177476894511, alpha=0.0625\nIteration=26, ||t_k - t_{k+1}||^2=2.701309413229415, alpha=0.125\nIteration=27, ||t_k - t_{k+1}||^2=3.037873680603465, alpha=0.125\nIteration=28, ||t_k - t_{k+1}||^2=2.238880472727269, alpha=0.125\nIteration=29, ||t_k - t_{k+1}||^2=1.3748842892157223, alpha=0.0625\nIteration=30, ||t_k - t_{k+1}||^2=1.0453572877049735, alpha=0.03125\nIteration=31, ||t_k - t_{k+1}||^2=1.760356495631824, alpha=0.125\nIteration=32, ||t_k - t_{k+1}||^2=2.002270246334261, alpha=0.125\nIteration=33, ||t_k - t_{k+1}||^2=1.9497449954932233, alpha=0.0625\nIteration=34, ||t_k - t_{k+1}||^2=1.3406662967578844, alpha=0.125\nIteration=35, ||t_k - t_{k+1}||^2=0.7210135281993513, alpha=0.03125\nIteration=36, ||t_k - t_{k+1}||^2=0.9357058909221085, alpha=0.0625\nIteration=37, ||t_k - t_{k+1}||^2=0.7444423081112751, alpha=0.03125\nIteration=38, ||t_k - t_{k+1}||^2=1.117463779448198, alpha=0.0625\nIteration=39, ||t_k - t_{k+1}||^2=0.013755707963922312, alpha=0.00048828125\nIteration=40, ||t_k - t_{k+1}||^2=1.3019840523270028, alpha=0.125\nMethod: xtilde. Terminated after 41 iterations, increment 3.0450843041331968e-12\n\n\n\n\"Completed in 1.034567673 secs\"","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"t = @elapsed xbar,increment_bar,loss_bar = newton_poisson_noise(G,y,z0,mu;numofiter = maxiter,tol=tol, method=\"xbar\",nrep=nrep)\ndisplay(\"Completed in $t secs\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"Iteration=0, ||t_k - t_{k+1}||^2=57.780858510888386, alpha=0.1\nIteration=1, ||t_k - t_{k+1}||^2=160.5310119066245, alpha=0.25\nIteration=2, ||t_k - t_{k+1}||^2=33.33509178856033, alpha=0.5\nIteration=3, ||t_k - t_{k+1}||^2=12.703984809288709, alpha=0.5\nIteration=4, ||t_k - t_{k+1}||^2=6.835088827951595, alpha=0.5\nIteration=5, ||t_k - t_{k+1}||^2=3.955408339689029, alpha=0.5\nIteration=6, ||t_k - t_{k+1}||^2=2.3718551871816693, alpha=0.5\nIteration=7, ||t_k - t_{k+1}||^2=1.4430581235048956, alpha=0.5\nIteration=8, ||t_k - t_{k+1}||^2=0.8969925976250477, alpha=0.5\nIteration=9, ||t_k - t_{k+1}||^2=0.559146754435279, alpha=0.5\nIteration=10, ||t_k - t_{k+1}||^2=0.36211424335209386, alpha=0.5\nIteration=11, ||t_k - t_{k+1}||^2=0.26841590415369393, alpha=0.5\nIteration=12, ||t_k - t_{k+1}||^2=0.15636145296730639, alpha=0.5\nIteration=13, ||t_k - t_{k+1}||^2=0.09826800502709797, alpha=0.5\nIteration=14, ||t_k - t_{k+1}||^2=0.06814231865385019, alpha=0.5\nIteration=15, ||t_k - t_{k+1}||^2=0.050018358475239395, alpha=0.5\nIteration=16, ||t_k - t_{k+1}||^2=0.03410437095466025, alpha=0.5\nIteration=17, ||t_k - t_{k+1}||^2=0.012470422084070986, alpha=0.25\nIteration=18, ||t_k - t_{k+1}||^2=0.016765114681943867, alpha=0.5\nIteration=19, ||t_k - t_{k+1}||^2=0.012729112026246305, alpha=0.5\nIteration=20, ||t_k - t_{k+1}||^2=0.004704722288952021, alpha=0.25\nIteration=21, ||t_k - t_{k+1}||^2=0.004974141026161668, alpha=0.5\nIteration=22, ||t_k - t_{k+1}||^2=0.003936879960652048, alpha=0.5\nIteration=23, ||t_k - t_{k+1}||^2=0.0024973579623034274, alpha=0.5\nIteration=24, ||t_k - t_{k+1}||^2=0.0019370854611915351, alpha=0.5\nIteration=25, ||t_k - t_{k+1}||^2=0.0006972774760610983, alpha=0.25\nIteration=26, ||t_k - t_{k+1}||^2=0.0008922284953580667, alpha=0.5\nIteration=27, ||t_k - t_{k+1}||^2=0.0007056683544332423, alpha=0.5\nIteration=28, ||t_k - t_{k+1}||^2=0.00026763661835611536, alpha=0.25\nIteration=29, ||t_k - t_{k+1}||^2=0.000370747228398961, alpha=0.5\nIteration=30, ||t_k - t_{k+1}||^2=0.00026544140858197494, alpha=0.5\nIteration=31, ||t_k - t_{k+1}||^2=0.00017119204363467755, alpha=0.5\nIteration=32, ||t_k - t_{k+1}||^2=0.00011802023300228027, alpha=0.5\nIteration=33, ||t_k - t_{k+1}||^2=8.795245141285355e-5, alpha=0.5\nIteration=34, ||t_k - t_{k+1}||^2=5.688014897753271e-5, alpha=0.5\nIteration=35, ||t_k - t_{k+1}||^2=4.500486121126704e-5, alpha=0.5\nIteration=36, ||t_k - t_{k+1}||^2=1.3291080441174658e-5, alpha=0.25\nIteration=37, ||t_k - t_{k+1}||^2=2.1412632232909954e-5, alpha=0.5\nIteration=38, ||t_k - t_{k+1}||^2=8.246432460896121e-6, alpha=0.25\nIteration=39, ||t_k - t_{k+1}||^2=1.6985263040973732e-5, alpha=0.5\nIteration=40, ||t_k - t_{k+1}||^2=5.610226713030864e-7, alpha=0.03125\nIteration=41, ||t_k - t_{k+1}||^2=2.0700113605276836e-9, alpha=0.0001220703125\nIteration=42, ||t_k - t_{k+1}||^2=2.4289739095342695e-7, alpha=0.015625\nIteration=43, ||t_k - t_{k+1}||^2=1.4707090358200456e-10, alpha=7.62939453125e-6\nIteration=44, ||t_k - t_{k+1}||^2=2.3451692235299514e-9, alpha=0.0001220703125\nMethod: xbar. Terminated after 45 iterations, increment 5.430883747370494e-11\n\n\n\n\"Completed in 3.110830348 secs\"","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"The forest-based updates is much faster w.r.t. the exact updates even though they take more iterations for satisfying the convergence criteria. Both qualitative and quantitative results show they approximate updates can give highly accurate results in much shorter amount of time!! ","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"y = rs(y/rescale)\nxhat = rs(xhat/rescale)\nxtilde = rs(xtilde/rescale)\nxbar = rs(xbar/rescale)\n\nfigure(figsize=[15,4])\nsubplot(1,5,1)\ntitle(\"Original Image\")\nimshow(Float64.(image),cmap=\"gray\")\naxis(\"off\")\nsubplot(1,5,2)\ntitle(\"Noisy Image\")\nimshow(im_noisy./rescale,cmap=\"gray\")\naxis(\"off\")\nsubplot(1,5,3)\ntitle(\"Solution by Exact Updates\")\nimshow(Float64.(xhat),cmap=\"gray\")\naxis(\"off\")\nsubplot(1,5,4)\ntitle(\"Solution by Forest Updates \")\nimshow(xtilde./rescale,cmap=\"gray\")\naxis(\"off\")\nsubplot(1,5,5)\ntitle(\"Solution by Forest Updates \")\nimshow(xbar./rescale,cmap=\"gray\")\naxis(\"off\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"(Image: png)","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"(-0.5, 99.5, 99.5, -0.5)","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"noisy_psnr = (ImageQualityIndexes.assess_psnr(y, image))\nxhat_psnr = (ImageQualityIndexes.assess_psnr(xhat, image))\nxtilde_psnr = (ImageQualityIndexes.assess_psnr(xtilde, image))\nxbar_psnr = (ImageQualityIndexes.assess_psnr(xbar, image))\n\ndisplay(\"y psnr: $noisy_psnr\")\ndisplay(\"xhat psnr: $xhat_psnr\")\ndisplay(\"xtilde psnr : $xtilde_psnr\")\ndisplay(\"xbar psnr : $xbar_psnr\")","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"\"y psnr: 17.384165847464146\"\n\n\n\n\"xhat psnr: 22.26589887507\"\n\n\n\n\"xtilde psnr : 22.144826892133786\"\n\n\n\n\"xbar psnr : 22.26589886523517\"","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"fig = figure(figsize=[5,5])\nplot((1:length(loss_tilde)), loss_tilde .+ logysum, color=\"blue\",linewidth=6.0, label=\"Updates with \\$ \\\\tilde{x}\\$\")\nplot((1:length(loss_bar)), loss_bar .+ logysum, color=\"orange\",linewidth=6.0, label=\"Updates with \\$ \\\\bar{x}\\$\")\nplot((1:length(loss_hat)), loss_hat .+ logysum, color=\"black\",linewidth=6.0, label=\"Updates with \\$ \\\\hat{x}\\$\")\nxlabel(\"Iterations\",fontsize=20)\nylabel(\"Loss Function\",fontsize=20)\nxticks(fontsize=15)\nyticks(fontsize=15)\nPyPlot.grid(true)\nxscale(\"log\")\nyscale(\"log\")\n\nlegend(fontsize=15)\ntight_layout()\n","category":"page"},{"location":"notebooks/Newton's method for Poisson noise/Newton's method for Poisson noise/","page":"Quasi-Newton's method by RSFs","title":"Quasi-Newton's method by RSFs","text":"(Image: png)","category":"page"},{"location":"typesandfunc/#Types-and-Functions","page":"Types and Functions","title":"Types and Functions","text":"","category":"section"},{"location":"typesandfunc/","page":"Types and Functions","title":"Types and Functions","text":"Modules = [KirchoffForests]\nOrder   = [:type, :function]","category":"page"},{"location":"typesandfunc/#Graphs.SimpleGraphs.SimpleDiGraph-Tuple{KirchoffForest}","page":"Types and Functions","title":"Graphs.SimpleGraphs.SimpleDiGraph","text":"SimpleDiGraph(rf::KirchoffForest)\n\nConvert a KirchoffForest rf to a SimpleDiGraph.\n\nExample\n\ng = grid([3,3])\nrf = random_forest(g,.4)\nf = SimpleDiGraph(rf)\nconnected_components(f)\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#Base.:*-Tuple{KirchoffForest, Matrix}","page":"Types and Functions","title":"Base.:*","text":"*(rf::KirchoffForest, Y::Matrix)\n\nTreating the random forest as a linear operator, propagate the value of y at the root to the rest of the tree.\n\nExample\n\ng = grid([5])\nrf = random_forest(g, .5)\nrf*collect(1:nv(g))\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#Base.:*-Tuple{Partition, Matrix}","page":"Types and Functions","title":"Base.:*","text":"*(p::Partition, Y::Matrix)\n\nTreating the graph partition as a linear operator, compute the average of Y over the partition.\n\nExample\n\ng = grid([5])\nrf = random_forest(g, .5)\np = Partition(rf)\np*collect(1:nv(g))\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.alias_draw-Tuple{PreprocessedWeightedGraph, Any}","page":"Types and Functions","title":"KirchoffForests.alias_draw","text":"Drawing procedure of alias method after the preprocessing Its cost is constant!\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.alias_preprocess-Tuple{SimpleWeightedGraphs.SimpleWeightedGraph}","page":"Types and Functions","title":"KirchoffForests.alias_preprocess","text":"This is the implementation of the preprocessing for the alias method.\nOverall cost for a graph is O(N^2)\nhttps://en.wikipedia.org/wiki/Alias_method\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.avg_rf-Tuple{Vector{Int64}, Vector{Float64}}","page":"Types and Functions","title":"KirchoffForests.avg_rf","text":"TODO: sum_by function is commented thus not in scope...\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.next-Tuple{KirchoffForest}","page":"Types and Functions","title":"KirchoffForests.next","text":"next(rf::KirchoffForest)\n\nReturn a vector of indices v, where v[i] = j means that node i points to node j in the forest. If v[i] = 0 i is a root.\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.random_forest-Tuple{Graphs.AbstractGraph, Any}","page":"Types and Functions","title":"KirchoffForests.random_forest","text":"random_forest(G::AbstractGraph,q)\n\nRun Wilson's algorithm on G to generate a random forest with parameter \"q\". q determines the probability that the random walk is interrupted at a node. If q is a scalar, that probability equals q/(q+d[i]) at node i with degree d[i]. If q is a vector, it equals q[i]/(q[i]+d[i]).\n\nExample\n\nusing Graphs G = grid([3,3]) random_forest(G,.4) q_varying = rand(nv(G)) rf = random_forest(G,q_varying) nroots(rf) next(rf) #who points to whom in the forest`\n\nTODO:\n\nAdd a rng parameter for reproducibility\nCode duplication for random_forest(G::AbstractGraph, q::AbstractFloat/AbstractVector), maybe define a function computing the acceptance to sink node\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.random_spanning_tree-Union{Tuple{T}, Tuple{Graphs.SimpleGraphs.SimpleGraph{T}, Integer}} where T","page":"Types and Functions","title":"KirchoffForests.random_spanning_tree","text":"random_spanning_tree(g, [r])\n\nGenerate a (uniform) random spanning tree (https://en.wikipedia.org/wiki/Spanning_tree) using Wilson's algorithm (https://dl.acm.org/doi/10.1145/237814.237880). A spanning tree of connected graph g is a connected subgraph of g that's cycle-free, i.e. a tree that includes only edges from g and connects every node. If you specify the root of the tree, the function produces a spanning tree that is picked uniformly among all trees rooted at r. If you do not specify a root, the function produces a random tree from g, picking uniformly among all spanning trees of g (over all possible roots).\n\nNB: a graph must be connected in order to have a spanning tree. By default, the function checks that g is connected (in order to avoid an infinite loop). If you are positive g is connected, use force=true.\n\nArguments\n\ng: a graph\noptional: r, index of a node to serve as root\nforce: if true, skip connectivity test\n\nOutput\n\nIf the root is specified, returns a tree, represented as a SimpleDiGraph. If it isn't, returns a named tuple with \"tree\": the tree and \"root\": the root.\n\nExamples\n\njulia> g = cycle_graph(4)\n{4, 4} undirected simple Int64 graph\n\njulia> random_spanning_tree(g).tree |> edges |> collect\n3-element Array{Graphs.SimpleGraphs.SimpleEdge{Int64},1}:\n Edge 2 => 3\n Edge 3 => 4\n Edge 4 => 1\n\nTODO: Maybe consider the function randomspanningtree as a meta function with a switch wilson / aldous..\n\n\n\n\n\n","category":"method"},{"location":"typesandfunc/#KirchoffForests.smooth-Union{Tuple{T}, Tuple{Graphs.AbstractGraph{T}, Vector, Any}} where T","page":"Types and Functions","title":"KirchoffForests.smooth","text":"smooth(g::AbstractGraph{T}, q, Y )\n\nSmooth signal over graph. Given a vector mathbfy of size nv(g), compute q(qmathbfI+mathbfL)^-1mathbfy, where mathbfL is the graph Laplacian and q > 0 is a regularisation coefficient (the smaller q, the stronger the smoothing).\n\nIf Y is a matrix then this function computes q(qmathbfI+mathbfL)^-1mathbfY. The linear system is solved using a direct method.\n\nExample\n\ng = grid([10])\nt = LinRange(0, 1, 10)\ny = sin.(6*pi*t)\nsmooth(g, .1, y)\nsmooth(g, 10.1, y)\n\n\n\n\n\n","category":"method"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/#ADMM-for-Edge-LASSO","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"","category":"section"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"The edge- LASSO (Least Absolute Shrinkage and Selection Operator) problem corresponds to L_1 regularization on graphs. The solution in this case is a sparse and smooth signal on the graph. It is formulated in the following way:  $ \t\\mathbf{x}^\\star = \\arg\\min{\\mathbf{x}\\in\\mathbb{R}^n} \\frac{q}{2}||\\mathbf{x} - \\mathbf{y}||2^2 + || \\mathsf{B}\\mathbf{x} ||1, $ where mathsfBinmathbbR^mtimes n is the edge-incidence matrix that takes the following form:  $     \\mathsf{B}{i,k} = \\begin{cases}         - w(i,j) & i< j \\text{ and } ek=(i,j)\\in\\mathcal{E} \\\n        w(i,j) & i> j  \\text{ and } ek=(i,j)\\in\\mathcal{E} \\          0 & \\text{otherwise}     \\end{cases}. $ In other words, mathsfBmathbfx computes the gradients of the signal mathbfx over the edges. Then, by taking the L_1 norm of this vector, one has the total absolute gradient of mathbfx. As aforementioned, minimization over this regularization cost yields a sparse and smooth signal. ","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"using KirchoffForests\nusing Graphs\nusing StatsBase\nusing Distributions\nusing SparseArrays\nusing LinearAlgebra\nusing PoissonRandom\nusing PyPlot","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"using TestImages, ImageQualityIndexes,Images","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"imname = \"house\"\nimage = imresize(testimage(imname), 128, 128)\nimage = Float64.(Gray.(image))\nnx = size(image,1)\nny = size(image,2)\nrs = (v) -> reshape(v,nx,ny)\nG = Graphs.grid([nx,ny])\nimage = image[:]\nnrep = 20\n\nσ = 0.2\nmu = 5.0","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"5.0","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"im_noisy = image .+ randn(length(image))*σ;","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/#Denoising-Gaussian-noise-via-ADMM-.","page":"ADMM for Edge LASSO","title":"Denoising Gaussian noise via ADMM .","text":"","category":"section"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"In this example, we consider an image denoising application where the noise is assumed to be additive Gaussian noise:  $     \\mathbf{y} = \\mathbf{x} + \\mathbf{\\epsilon}, \\quad \\forall i \\in \\mathcal{V}, \\epsiloni \\sim\\mathcal{N}(\\mathbf{0},\\mathsf{I})  $ Then, for the problem above, one can approximate the solution by using the method of ADMM. This method iteratively solves the problem at hand by running the iterative steps:  $ \\begin{split} \t\\mathbf{x}{k+1} &= \\arg\\min{\\mathbf{x} \\in\\mathbb{R}^n}\\left(\\frac{q}{2}||\\mathbf{x}-\\mathbf{y}||2^2 + \\frac{\\rho}{2}||\\mathsf{B}\\mathbf{x} -\\mathbf{z}{k}+\\mathbf{u}{k}||2^2\\right) \\\n\t\\mathbf{z}{k+1} &= \\arg\\min{\\mathbf{z} \\in\\mathbb{R}^m}\\left(||\\mathbf{z}||1 + \\frac{\\rho}{2}||\\mathsf{B}\\mathbf{x}{k+1} -\\mathbf{z}+\\mathbf{u}{k}||2^2 \\right) \\\n\t\\mathbf{u}{k+1} &= \\mathbf{u}k + (\\mathsf{B}\\mathbf{x}^{k+1} - \\mathbf{z}^{k+1}). \\\n\\end{split} $ The parameters \\mathbf{x}k mathbfy_k and mathbfu_k are arbitrarily initialized and are updated at every step, and rho is a user-defined parameter. Here, the computationally heavy part is the first step in which one needs to compute this inverse:  $     \\mathbf{x}{k+1} = (q\\mathsf{I} + \\rho\\mathsf{L})^{-1}(q\\mathbf{y} + \\rho\\mathsf{B}^{\\top}\\mathbf{z}k - \\rho\\mathsf{B}^\\top\\mathbf{u}_k). $ Notice that one can see this inverse as the regularized inverse of mathsfL, thus it can be approximated via forest-based estimators. Moreover, the updated part through iterations is not the inverse but the vector that the inverted matrix is multiplied with. This allows us to sample spanning forests once and use them for every iterations. This idea has not implemented yet, so the following function implements ADMM with the exact and forest based updates while forests are sampled at every iterations.  ","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"@elapsed xadmm_exact,loss_func_exact= admm_edge_lasso(G,mu,im_noisy;maxiter=200,ρ=0.2,method=\"exact\")","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"25.171844969","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"@elapsed xadmm_forest,loss_func_forest= admm_edge_lasso(G,mu,im_noisy;maxiter=200,ρ=0.2,method=\"xbar\",Nfor=3)","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"4.580849309","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"xadmm_exact= rs(xadmm_exact)\nxadmm_forest= rs(xadmm_forest);","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"Both qualitative and quantitative result shows that forest-based updates empirically converges the solution computed by the exact updates while taking much less time. Moreover, the sparse (almost piece-wise constant) structure of the solution is clearly visible on these image examples.","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"figure(figsize=[15,4])\nsubplot(1,4,1)\ntitle(\"Original Image\")\nimshow(rs(image),cmap=\"gray\")\naxis(\"off\")\nsubplot(1,4,2)\ntitle(\"Noisy Image\")\nimshow(rs(im_noisy),cmap=\"gray\")\naxis(\"off\")\nsubplot(1,4,3)\ntitle(\"Solution by Exact Updates\")\nimshow(xadmm_exact,cmap=\"gray\")\naxis(\"off\")\nsubplot(1,4,4)\ntitle(\"Solution by Forest Updates \")\nimshow(rs(xadmm_forest),cmap=\"gray\")\naxis(\"off\")\n","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"(Image: png)","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"(-0.5, 127.5, 127.5, -0.5)","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"y = rs(im_noisy)\nnoisy_psnr = (ImageQualityIndexes.assess_psnr(y, image))\nxadmm_exact_psnr = (ImageQualityIndexes.assess_psnr(xadmm_exact, image))\nxforest_psnr = (ImageQualityIndexes.assess_psnr(xadmm_forest, image))\n\ndisplay(\"y psnr: $noisy_psnr\")\ndisplay(\"xadmm_exact psnr: $xadmm_exact_psnr\")\ndisplay(\"xforest psnr : $xforest_psnr\")","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"\"y psnr: 13.972511184499886\"\n\n\n\n\"xadmm_exact psnr: 23.860703046152313\"\n\n\n\n\"xforest psnr : 23.816177942582524\"","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"fig = figure(figsize=[5,5])\nplot((1:length(loss_func_exact)), loss_func_exact, color=\"black\",linewidth=6.0, label=string(\"Updates with \", latexstring(\"\\$\\\\hat{x}\\$\")))\nplot((1:length(loss_func_forest)), loss_func_forest, color=\"orange\",linewidth=6.0,linestyle=\"--\", label=string(\"Updates with \", latexstring(\"\\$\\\\bar{x}\\$\")))\nxlabel(\"Iterations\",fontsize=20)\nylabel(\"Loss Function\",fontsize=20)\nxticks(fontsize=15)\nyticks(fontsize=15)\nPyPlot.grid(true)\ntight_layout()\nlegend(fontsize=15)\n","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"(Image: png)","category":"page"},{"location":"notebooks/Edge Lasso- ADMM/Edge Lasso- ADMM/","page":"ADMM for Edge LASSO","title":"ADMM for Edge LASSO","text":"PyObject <matplotlib.legend.Legend object at 0x7f5b5004c460>","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"using Plots,Graphs,KirchoffForests,Random\npyplot()\n\ng = Graphs.grid([4,4])\np = Iterators.product(0.0:0.1:0.3, 0.0:0.1:0.3);\nxloc = zeros(nv(g))\nyloc = zeros(nv(g))\nglobal i = 0\nfor (x,y) in p\n  global i += 1\n  xloc[i] = x\n  yloc[i] = y  \nend\n\ngraphplotobj = RFGraphPlot(g,xloc,yloc,repeat([1.0],nv(g)),15,3,10,:viridis,false,\"\",15,15,\"\")\nplot(graphplotobj)\nsavefig(\"ex_graph.svg\")\n\nrt = random_spanning_tree(g)\ntreeplotobj = RFGraphPlot(rt.tree,xloc,yloc,[(i == rt.root) for i = 1:nv(g)],15,3,1.2,:viridis,false,\"\",15,15,\"\")\nplot(treeplotobj)\nsavefig(\"ex_tree.svg\")\n\nrng = MersenneTwister(12)\nrf = random_forest(g,1.0,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,[(i in rf.roots) for i = 1:nv(g)],15,3,1.2,:viridis,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"ex_forest.svg\")\n","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"using Graphs,KirchoffForests,Plots,Random\npyplot()\n\ng = Graphs.grid([4,4])\np = Iterators.product(0.0:0.1:0.3, 0.0:0.1:0.3);\nxloc = zeros(nv(g))\nyloc = zeros(nv(g))\nglobal i = 0\nfor (x,y) in p\n  global i += 1\n  xloc[i] = x\n  yloc[i] = y  \nend\n\nrng = MersenneTwister(12)\nrf = random_forest(g,0.1,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,[(i in rf.roots) for i = 1:nv(g)],15,3,1.2,:viridis,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"q=0.1.svg\")\n\nrf = random_forest(g,1.0,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,[(i in rf.roots) for i = 1:nv(g)],15,3,1.2,:viridis,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"q=1.0.svg\")\n\nrf = random_forest(g,5.0,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,[(i in rf.roots) for i = 1:nv(g)],15,3,1.2,:viridis,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"q=5.0.svg\")\n\n","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"using Graphs,Plots,KirchoffForests,Random\npyplot()\n \ng = Graphs.grid([4,4])\np = Iterators.product(0.0:0.1:0.3, 0.0:0.1:0.3);\nxloc = zeros(nv(g))\nyloc = zeros(nv(g))\nglobal i = 0\nfor (x,y) in p\n  global i += 1\n  xloc[i] = x\n  yloc[i] = y  \nend\nrng = MersenneTwister(12)\nrf = random_forest(g,1.0,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,repeat([1.0],nv(g)),15,3,1.2,:jet,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"quniform.svg\")\n\nidx = [1,4,13,16]\nq = 0.02*ones(nv(g))\nq[idx] .= (16-12*0.02)/4\nrf = random_forest(g,q,rng)\nforestplotobj = RFGraphPlot(SimpleDiGraph(rf),xloc,yloc,q,15,3,1.2,:jet,false,\"\",15,15,\"\")\nplot(forestplotobj)\nsavefig(\"qnonuniform.svg\")\n","category":"page"},{"location":"#KirchoffForests.jl:-a-Julia-package-for-Random-Forests-on-Graphs,-and-Applications","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"","category":"section"},{"location":"#Welcome-to-the-Documentation-of-KirchoffForest.jl","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"Welcome to the Documentation of KirchoffForest.jl","text":"","category":"section"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"A random spanning forest (RSF) is a special random object on graph/networks which has elegant theoretical links with graph Laplacians and wide set of applications in graph signal processing and machine learning.  ","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"This package is dedicated to implementing:","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"Sampling algorithms for RSFs\nRandomized algorithms for various problems including:\nGraph Signal Interpolation and Tikhonov regularization\nTrace Estimation\nAnd more to come...","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"warning: Warning\nThe random forests produced by this package come from graph theory and are unrelated to the random forests found in machine learning.","category":"page"},{"location":"#Installation-Instructions","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"Installation Instructions","text":"","category":"section"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"The package is not registered yet. Therefore, you can install it as follows:","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"julia> ] add https://gricad-gitlab.univ-grenoble-alpes.fr/barthesi/RandomForests.jl","category":"page"},{"location":"#Table-of-Contents","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"Apart from the usual documentation for the types and functions, some theoretical materials along with practical examples are provided to illustrate the full scope of RandomForest.jl.","category":"page"},{"location":"","page":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","title":"KirchoffForests.jl: a Julia package for Random Forests on Graphs, and Applications","text":"Pages = [\"rsf.md\",\"gtr.md\",\"trace.md\",\"typesandfunc.md\"]\nDepth = 5","category":"page"}]
}
