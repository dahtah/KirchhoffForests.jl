<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Trace Estimation · RandomForests.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">RandomForests.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">RandomForests.jl: a Julia package for Random Forests on Graphs, and Applications</a></li><li><a class="tocitem" href="../rsf/">What&#39;s an RSF?</a></li><li><a class="tocitem" href="../gtr/">RSF-based Graph Tikhonov Regularization</a></li><li class="is-active"><a class="tocitem" href>Trace Estimation</a><ul class="internal"><li><a class="tocitem" href="#Variance-Reduction"><span>Variance Reduction</span></a></li></ul></li><li><a class="tocitem" href="../typesandfunc/">Types and Functions</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Trace Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Trace Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/master/docs/src/trace.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Trace-estimation-via-RSFs"><a class="docs-heading-anchor" href="#Trace-estimation-via-RSFs">Trace estimation via RSFs</a><a id="Trace-estimation-via-RSFs-1"></a><a class="docs-heading-anchor-permalink" href="#Trace-estimation-via-RSFs" title="Permalink"></a></h1><p>Trace, i.e. the sum of diagonal entries of an input matrix, is an essential operation in linear algebra and it is of central importance in many applications in machine learning. However, it is not trivial to compute when the diagonals of the input matrix are expensive to compute.</p><p>Here we give efficient RSF-based estimators for approximating the trace of regularized inverse <span>$\text{tr}(\mathsf{K})$</span> which normally requires computing the inverse of a large matrix. They are unbiased, easy to implement and parallelise and can be used for the regularized inverse of symmetric diagonally dominant matrices instead of graph Laplacians. See <a href="../references/#Barthelme2019">Simon Barthelmé, Nicolas Tremblay, Alexandre Gaudillière, Luca Avena, Pierre-Olivier Amblard (2019)</a> for more details.</p><div class="admonition is-info"><header class="admonition-header">RSF-based trace estimator</header><div class="admonition-body"><p>The proposed estimator <span>$s$</span> is the number of roots in <span>$\Phi_Q$</span>:</p><p class="math-container">\[    s \coloneqq |\rho(\Phi_Q)|,\]</p><p>which verifies <span>$\mathbb{E}[s] = \text{tr}(\mathsf{K})$</span>.</p></div></div><p>One can access the number of roots in the sampled forest as follows:</p><pre><code class="language- hljs">julia&gt; using RandomForests,Graphs

julia&gt; g = grid([4,4])
{16, 24} undirected simple Int64 graph

julia&gt; q = 1.0
1.0

julia&gt; rf = random_forest(g,q)
Random forest. Size of original graph 16.
Number of trees 8


julia&gt; s = rf.nroots
8</code></pre><h2 id="Variance-Reduction"><a class="docs-heading-anchor" href="#Variance-Reduction">Variance Reduction</a><a id="Variance-Reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Variance-Reduction" title="Permalink"></a></h2><p>By applying standard variance reduction techniques in Monte Carlo literature, one reduces the expected error/variance of this estimator.</p><h3 id="Control-Variate"><a class="docs-heading-anchor" href="#Control-Variate">Control Variate</a><a id="Control-Variate-1"></a><a class="docs-heading-anchor-permalink" href="#Control-Variate" title="Permalink"></a></h3><p>By adapting the control variate in <a href="../gtr/">GTR</a>, one obtains the following estimators:</p><p class="math-container">\[    \tilde{s}\coloneqq s - α*(\text{tr}(\mathsf{K}^{-1}\tilde{\mathsf{S}}) - s) \\ 
    
    \bar{s}\coloneqq s - α*(\text{tr}(\mathsf{K}^{-1}\bar{\mathsf{S}}) - s)\]</p><p>where <span>$\tilde{\mathsf{S}}$</span> and <span>$\bar{\mathsf{S}}$</span> are typically low-rank linear operators that verifies <span>$\tilde{\mathbf{x}} = \tilde{\mathsf{S}}\mathbf{y}$</span> and <span>$\bar{\mathbf{x}} = \bar{\mathsf{S}}\mathbf{y}$</span>, respectively. In other words, when applied to a vector, <span>$\tilde{\mathsf{S}}$</span> propagates the measurements in the roots and <span>$\bar{\mathsf{S}}$</span> propagates the average measurement within each tree. One can choose <span>$\alpha$</span> as suggested in <a href="../gtr/">GTR</a> and for more details, see <a href="../references/#pilavci2022variance">Yusuf Yiğit Pilavcı, Pierre-Olivier Amblard, Simon Barthelmé, Nicolas Tremblay (2022)</a>.</p><h3 id="Stratification"><a class="docs-heading-anchor" href="#Stratification">Stratification</a><a id="Stratification-1"></a><a class="docs-heading-anchor-permalink" href="#Stratification" title="Permalink"></a></h3><p>Another variance reduction technique we can use for trace estimation is stratification. Imagine a random variable <span>$X$</span> with an unknown expectation <span>$\mu$</span>. This technique suggests:</p><p>1 - Collecting <span>$N_k$</span> conditional samples of <span>$X|Y\in C_k$</span> for each <span>$k\in\{1,\dots,K\}$</span> where <span>$Y$</span> is another random variable with a known probability distribution over its sample space <span>$\Omega =\cup_{k=1}^{K} C_k$</span>,</p><p>2 - Then approximating <span>$\mu$</span> by using the law of iterated expectation as follows: </p><p class="math-container">\[    \mu_{st} \coloneqq \underbrace{\sum_{k=1}^K\underbrace{{\frac{1}{N_k}\left(\sum_{j=1 }^{N_k} X^{(j)}|Y\in C_k  \right)}}_{\text{Conditional Expectation}}\mathbb{P}(Y\in C_i)}_{\text{Marginalization over } Y}.\]</p><p>For certain allocations of samples <span>$N_k$</span>&#39;s, the theoretical variance of <span>$\mu_{st}$</span> is less than that of the naive estimator. We can build such a setup for trace estimator <span>$s$</span> by setting <span>$Y$</span> to &quot;the roots that are sampled at the first visits&quot; of random walks in Wilson&#39;s algorithm. These roots are a subset of <span>$\rho(\Phi_q)$</span> and their distribution (Poisson-Binom distribution) is tractable by using the degrees of nodes. More details can be found in <a href="../references/#pilavcithesis">Yusuf Yigit Pilavci (2022)</a>. </p><p>All of these techniques are implemented in <a href="../typesandfunc/#RandomForests.trace_estimator-Tuple{Graphs.AbstractGraph, Real}"><code>trace_estimator</code></a> as different variants. Here is an example comparing all these methods: </p><pre><code class="language-julia hljs">using RandomForests,Graphs,LinearAlgebra,PyPlot, StatsBase
pygui(true)
n = 10000
g = barabasi_albert(n,10)
L = Matrix(laplacian_matrix(g))
q = 1.0
tr_exact = tr(q*inv(L+q*I)) 

α=2*q/(2*q+mean(degree(g)))
NREPRANGE = Int64.(round.(10 .^ (1:0.05:2)))

err_tr_est = zeros(length(NREPRANGE))
err_tr_cv_tilde = zeros(length(NREPRANGE))
err_tr_cv_bar = zeros(length(NREPRANGE))
err_tr_st = zeros(length(NREPRANGE))

EXPREP = 1000
for er = 1 : EXPREP
    for (idx,NREP) in enumerate(NREPRANGE)
        tr_est = trace_estimator(g,q;variant=1,NREP=NREP) # The forest estimator s
        err_tr_est[idx] += (tr_est - tr_exact)^2
        tr_est_cv_tilde = trace_estimator(g,q;variant=2,α=α,NREP=NREP) # s_tilde 
        err_tr_cv_tilde[idx] += (tr_est_cv_tilde - tr_exact)^2
        tr_est_cv_bar = trace_estimator(g,q;variant=3,α=α,NREP=NREP) # s_bar
        err_tr_cv_bar[idx] += (tr_est_cv_bar - tr_exact)^2
        tr_est_st = trace_estimator(g,q;variant=4,α=α,NREP=NREP) # stratified estimator s_st
        err_tr_st[idx] += (tr_est_st - tr_exact)^2
    end
end

plot(NREPRANGE,err_tr_est ./ EXPREP,label=&quot;s&quot;)
plot(NREPRANGE,err_tr_cv_tilde ./ EXPREP,label=&quot;\$ \\tilde{s}\$&quot;)
plot(NREPRANGE,err_tr_cv_bar ./ EXPREP,label=&quot;\$ \\bar{s}\$&quot;)
plot(NREPRANGE,err_tr_st ./ EXPREP,label=&quot;\$ s_{st}\$&quot;)

xlabel(&quot;Number of Samples&quot;)
ylabel(&quot;Mean Square Error&quot;)
yscale(&quot;log&quot;)
xscale(&quot;log&quot;)
legend()</code></pre><p><img src="../trace_comp.png" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gtr/">« RSF-based Graph Tikhonov Regularization</a><a class="docs-footer-nextpage" href="../typesandfunc/">Types and Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Friday 14 July 2023 22:41">Friday 14 July 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
